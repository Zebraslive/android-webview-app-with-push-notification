{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Train your own GAN in a few lines of code!\nSee original StyleGAN2 ADA github repo here: https://github.com/NVlabs/stylegan2-ada-pytorch\n\nBy using a pre-trained model along with data heavy data augmentation, we can train our own GAN with a very limited dataset (<1000 images). \n\nMake sure you have a GPU runtime!","metadata":{}},{"cell_type":"code","source":"!pip install pyspng ninja imageio-ffmpeg==0.4.3\n#!git clone https://github.com/NVlabs/stylegan2-ada-pytorch\n!pip install gdown \n%cd /kaggle/working/stylegan2-ada-pytorch","metadata":{"execution":{"iopub.status.busy":"2023-01-09T03:32:44.393677Z","iopub.execute_input":"2023-01-09T03:32:44.394089Z","iopub.status.idle":"2023-01-09T03:33:04.511788Z","shell.execute_reply.started":"2023-01-09T03:32:44.394025Z","shell.execute_reply":"2023-01-09T03:33:04.510913Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"Collecting pyspng\n  Downloading pyspng-0.1.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (207 kB)\n     |████████████████████████████████| 207 kB 2.0 MB/s            \n\u001b[?25hCollecting ninja\n  Downloading ninja-1.11.1-py2.py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (145 kB)\n     |████████████████████████████████| 145 kB 24.3 MB/s            \n\u001b[?25hCollecting imageio-ffmpeg==0.4.3\n  Downloading imageio_ffmpeg-0.4.3-py3-none-manylinux2010_x86_64.whl (26.9 MB)\n     |████████████████████████████████| 26.9 MB 35.6 MB/s            \n\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from pyspng) (1.20.3)\nInstalling collected packages: pyspng, ninja, imageio-ffmpeg\nSuccessfully installed imageio-ffmpeg-0.4.3 ninja-1.11.1 pyspng-0.1.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\nCollecting gdown\n  Downloading gdown-4.6.0-py3-none-any.whl (14 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from gdown) (3.4.2)\nRequirement already satisfied: requests[socks] in /opt/conda/lib/python3.7/site-packages (from gdown) (2.26.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from gdown) (1.16.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from gdown) (4.62.3)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.7/site-packages (from gdown) (4.10.0)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.7/site-packages (from beautifulsoup4->gdown) (2.3.1)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (2021.10.8)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (1.26.7)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (2.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (3.1)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (1.7.1)\nInstalling collected packages: gdown\nSuccessfully installed gdown-4.6.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n/kaggle/working/stylegan2-ada-pytorch\n","output_type":"stream"}]},{"cell_type":"code","source":"%cd /kaggle/working/\n!git clone https://github.com/stylegan-human/StyleGAN-Human\n!wget 'https://cdn-lfs.huggingface.co/repos/1a/8c/1a8c83e1fa088d576d8201186ec45a38de5bcc3dfdbafcea86e5b3afa9362d1b/7af33f638e3b3aba7bf99456eec3e9d4a022d8a7dd67683a3605a7dd37665a3b?response-content-disposition=attachment%3B%20filename%3D%22stylegan_human_v2_512.pkl%22&Expires=1673415751&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzFhLzhjLzFhOGM4M2UxZmEwODhkNTc2ZDgyMDExODZlYzQ1YTM4ZGU1YmNjM2RmZGJhZmNlYTg2ZTViM2FmYTkzNjJkMWIvN2FmMzNmNjM4ZTNiM2FiYTdiZjk5NDU2ZWVjM2U5ZDRhMDIyZDhhN2RkNjc2ODNhMzYwNWE3ZGQzNzY2NWEzYj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPWF0dGFjaG1lbnQlM0IlMjBmaWxlbmFtZSUzRCUyMnN0eWxlZ2FuX2h1bWFuX3YyXzUxMi5wa2wlMjIiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2NzM0MTU3NTF9fX1dfQ__&Signature=dHvc7yaTAUB5NtA1vwnogSn4LXxufWERMQ4HJviNMs-VF-ZhzDTdGGIKC7m10yvRnL1QyIALkiIc~4JJMBiQS1Q0ghzM5ai3bNBnUOiiKkw~ZLNpoy3~4B3SXly5tgF8qZKDHkirna5IFyvLLd~mAbpubhGC1MSoHdv5spYQE4DSbyPFWRjz6LAz4Qp9TzGMN~s93gMi-eGA08DttzNXGT4n7G4RQl0YMJvcc-vVoBW7OjlS8yIAZpAlE~05bGQY9p8ECviHFz-FCQJ5TKn4NbCrtAKoXsRz9OjTtIXczD7Jg4oNIW9qXq3vZIAg~awElidutrX925BeP5zuTjIZRg__&Key-Pair-Id=KVTP0A1DKRTAX' -O /kaggle/working/styleganhuman.pkl","metadata":{"execution":{"iopub.status.busy":"2023-01-08T05:45:05.025917Z","iopub.execute_input":"2023-01-08T05:45:05.026774Z","iopub.status.idle":"2023-01-08T05:45:23.190615Z","shell.execute_reply.started":"2023-01-08T05:45:05.026734Z","shell.execute_reply":"2023-01-08T05:45:23.189727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cp /kaggle/working/StyleGAN-Human/training_scripts/sg2/training/networks.py /kaggle/working/stylegan2-ada-pytorch/training/networks.py","metadata":{"execution":{"iopub.status.busy":"2023-01-08T06:25:52.620928Z","iopub.execute_input":"2023-01-08T06:25:52.621249Z","iopub.status.idle":"2023-01-08T06:25:53.606346Z","shell.execute_reply.started":"2023-01-08T06:25:52.621216Z","shell.execute_reply":"2023-01-08T06:25:53.605149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd /kaggle/working/stylegan2-ada-pytorch","metadata":{"execution":{"iopub.status.busy":"2023-01-09T02:56:37.621885Z","iopub.execute_input":"2023-01-09T02:56:37.622636Z","iopub.status.idle":"2023-01-09T02:56:37.634968Z","shell.execute_reply.started":"2023-01-09T02:56:37.622591Z","shell.execute_reply":"2023-01-09T02:56:37.634126Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"/kaggle/working/stylegan2-ada-pytorch\n","output_type":"stream"}]},{"cell_type":"code","source":"!python trainh.py --outdir=training_results/sg2/ --metrics=None --data=/kaggle/input/human512-196imgs/ --gpus=2 --aug=noaug --mirror=1 --snap=10 --cfg=shhq --square=False --resume=training_results/sg2/00007-croppedhumans-rectangle-mirror-shhq2-noaug-resumecustom/network-snapshot-000080.pkl","metadata":{"execution":{"iopub.status.busy":"2023-01-09T03:33:09.752687Z","iopub.execute_input":"2023-01-09T03:33:09.752965Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"square :  False\ndesc:  \n\nTraining options:\n{\n  \"num_gpus\": 2,\n  \"image_snapshot_ticks\": 10,\n  \"network_snapshot_ticks\": 10,\n  \"metrics\": [],\n  \"random_seed\": 0,\n  \"training_set_kwargs\": {\n    \"class_name\": \"training.dataset.ImageFolderDataset\",\n    \"path\": \"/kaggle/input/human512-196imgs/\",\n    \"use_labels\": false,\n    \"max_size\": 197,\n    \"xflip\": true,\n    \"square\": false,\n    \"resolution\": 512\n  },\n  \"data_loader_kwargs\": {\n    \"pin_memory\": true,\n    \"num_workers\": 3,\n    \"prefetch_factor\": 2\n  },\n  \"G_kwargs\": {\n    \"class_name\": \"training.networks.Generator\",\n    \"z_dim\": 512,\n    \"w_dim\": 512,\n    \"mapping_kwargs\": {\n      \"num_layers\": 8\n    },\n    \"synthesis_kwargs\": {\n      \"channel_base\": 32768,\n      \"channel_max\": 512,\n      \"num_fp16_res\": 4,\n      \"conv_clamp\": 256\n    },\n    \"square\": false\n  },\n  \"D_kwargs\": {\n    \"class_name\": \"training.networks.Discriminator\",\n    \"block_kwargs\": {},\n    \"mapping_kwargs\": {},\n    \"epilogue_kwargs\": {\n      \"mbstd_group_size\": 4\n    },\n    \"square\": false,\n    \"channel_base\": 32768,\n    \"channel_max\": 512,\n    \"num_fp16_res\": 4,\n    \"conv_clamp\": 256\n  },\n  \"G_opt_kwargs\": {\n    \"class_name\": \"torch.optim.Adam\",\n    \"lr\": 0.0025,\n    \"betas\": [\n      0,\n      0.99\n    ],\n    \"eps\": 1e-08\n  },\n  \"D_opt_kwargs\": {\n    \"class_name\": \"torch.optim.Adam\",\n    \"lr\": 0.0025,\n    \"betas\": [\n      0,\n      0.99\n    ],\n    \"eps\": 1e-08\n  },\n  \"loss_kwargs\": {\n    \"class_name\": \"training.loss.StyleGAN2Loss\",\n    \"r1_gamma\": 3.2768\n  },\n  \"total_kimg\": 25000,\n  \"batch_size\": 16,\n  \"batch_gpu\": 8,\n  \"ema_kimg\": 5.0,\n  \"ema_rampup\": null,\n  \"resume_pkl\": \"training_results/sg2/00007-croppedhumans-rectangle-mirror-shhq2-noaug-resumecustom/network-snapshot-000080.pkl\",\n  \"ada_kimg\": 100,\n  \"run_dir\": \"training_results/sg2/00014--rectangle-mirror-shhq2-noaug-resumecustom\"\n}\n\nOutput directory:   training_results/sg2/00014--rectangle-mirror-shhq2-noaug-resumecustom\nTraining data:      /kaggle/input/human512-196imgs/\nTraining duration:  25000 kimg\nNumber of GPUs:     2\nNumber of images:   197\nImage resolution:   512\nConditional model:  False\nDataset x-flips:    True\n\nCreating output directory...\nLaunching processes...\nLoading training set...\n/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  cpuset_checked))\n/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  cpuset_checked))\n\nNum images:  394\nImage shape: [3, 512, 256]\nLabel shape: [0]\n\nConstructing networks...\nResuming from \"training_results/sg2/00007-croppedhumans-rectangle-mirror-shhq2-noaug-resumecustom/network-snapshot-000080.pkl\"\nSetting up PyTorch plugin \"bias_act_plugin\"... Done.\nSetting up PyTorch plugin \"upfirdn2d_plugin\"... Done.\n\nGenerator             Parameters  Buffers  Output shape        Datatype\n---                   ---         ---      ---                 ---     \nmapping.fc0           262656      -        [8, 512]            float32 \nmapping.fc1           262656      -        [8, 512]            float32 \nmapping.fc2           262656      -        [8, 512]            float32 \nmapping.fc3           262656      -        [8, 512]            float32 \nmapping.fc4           262656      -        [8, 512]            float32 \nmapping.fc5           262656      -        [8, 512]            float32 \nmapping.fc6           262656      -        [8, 512]            float32 \nmapping.fc7           262656      -        [8, 512]            float32 \nmapping               -           512      [8, 16, 512]        float32 \nsynthesis.b4.conv1    2622465     24       [8, 512, 4, 2]      float32 \nsynthesis.b4.torgb    264195      -        [8, 3, 4, 2]        float32 \nsynthesis.b4:0        4096        16       [8, 512, 4, 2]      float32 \nsynthesis.b4:1        -           -        [8, 512, 4, 2]      float32 \nsynthesis.b8.conv0    2622465     48       [8, 512, 8, 4]      float32 \nsynthesis.b8.conv1    2622465     48       [8, 512, 8, 4]      float32 \nsynthesis.b8.torgb    264195      -        [8, 3, 8, 4]        float32 \nsynthesis.b8:0        -           16       [8, 512, 8, 4]      float32 \nsynthesis.b8:1        -           -        [8, 512, 8, 4]      float32 \nsynthesis.b16.conv0   2622465     144      [8, 512, 16, 8]     float32 \nsynthesis.b16.conv1   2622465     144      [8, 512, 16, 8]     float32 \nsynthesis.b16.torgb   264195      -        [8, 3, 16, 8]       float32 \nsynthesis.b16:0       -           16       [8, 512, 16, 8]     float32 \nsynthesis.b16:1       -           -        [8, 512, 16, 8]     float32 \nsynthesis.b32.conv0   2622465     528      [8, 512, 32, 16]    float32 \nsynthesis.b32.conv1   2622465     528      [8, 512, 32, 16]    float32 \nsynthesis.b32.torgb   264195      -        [8, 3, 32, 16]      float32 \nsynthesis.b32:0       -           16       [8, 512, 32, 16]    float32 \nsynthesis.b32:1       -           -        [8, 512, 32, 16]    float32 \nsynthesis.b64.conv0   2622465     2064     [8, 512, 64, 32]    float16 \nsynthesis.b64.conv1   2622465     2064     [8, 512, 64, 32]    float16 \nsynthesis.b64.torgb   264195      -        [8, 3, 64, 32]      float16 \nsynthesis.b64:0       -           16       [8, 512, 64, 32]    float16 \nsynthesis.b64:1       -           -        [8, 512, 64, 32]    float32 \nsynthesis.b128.conv0  1442561     8208     [8, 256, 128, 64]   float16 \nsynthesis.b128.conv1  721409      8208     [8, 256, 128, 64]   float16 \nsynthesis.b128.torgb  132099      -        [8, 3, 128, 64]     float16 \nsynthesis.b128:0      -           16       [8, 256, 128, 64]   float16 \nsynthesis.b128:1      -           -        [8, 256, 128, 64]   float32 \nsynthesis.b256.conv0  426369      32784    [8, 128, 256, 128]  float16 \nsynthesis.b256.conv1  213249      32784    [8, 128, 256, 128]  float16 \nsynthesis.b256.torgb  66051       -        [8, 3, 256, 128]    float16 \nsynthesis.b256:0      -           16       [8, 128, 256, 128]  float16 \nsynthesis.b256:1      -           -        [8, 128, 256, 128]  float32 \nsynthesis.b512.conv0  139457      131088   [8, 64, 512, 256]   float16 \nsynthesis.b512.conv1  69761       131088   [8, 64, 512, 256]   float16 \nsynthesis.b512.torgb  33027       -        [8, 3, 512, 256]    float16 \nsynthesis.b512:0      -           16       [8, 64, 512, 256]   float16 \nsynthesis.b512:1      -           -        [8, 64, 512, 256]   float32 \n---                   ---         ---      ---                 ---     \nTotal                 30272487    350392   -                   -       \n\n\nDiscriminator  Parameters  Buffers  Output shape        Datatype\n---            ---         ---      ---                 ---     \nb512.fromrgb   256         16       [8, 64, 512, 256]   float16 \nb512.skip      8192        16       [8, 128, 256, 128]  float16 \nb512.conv0     36928       16       [8, 64, 512, 256]   float16 \nb512.conv1     73856       16       [8, 128, 256, 128]  float16 \nb512           -           16       [8, 128, 256, 128]  float16 \nb256.skip      32768       16       [8, 256, 128, 64]   float16 \nb256.conv0     147584      16       [8, 128, 256, 128]  float16 \nb256.conv1     295168      16       [8, 256, 128, 64]   float16 \nb256           -           16       [8, 256, 128, 64]   float16 \nb128.skip      131072      16       [8, 512, 64, 32]    float16 \nb128.conv0     590080      16       [8, 256, 128, 64]   float16 \nb128.conv1     1180160     16       [8, 512, 64, 32]    float16 \nb128           -           16       [8, 512, 64, 32]    float16 \nb64.skip       262144      16       [8, 512, 32, 16]    float16 \nb64.conv0      2359808     16       [8, 512, 64, 32]    float16 \nb64.conv1      2359808     16       [8, 512, 32, 16]    float16 \nb64            -           16       [8, 512, 32, 16]    float16 \nb32.skip       262144      16       [8, 512, 16, 8]     float32 \nb32.conv0      2359808     16       [8, 512, 32, 16]    float32 \nb32.conv1      2359808     16       [8, 512, 16, 8]     float32 \nb32            -           16       [8, 512, 16, 8]     float32 \nb16.skip       262144      16       [8, 512, 8, 4]      float32 \nb16.conv0      2359808     16       [8, 512, 16, 8]     float32 \nb16.conv1      2359808     16       [8, 512, 8, 4]      float32 \nb16            -           16       [8, 512, 8, 4]      float32 \nb8.skip        262144      16       [8, 512, 4, 2]      float32 \nb8.conv0       2359808     16       [8, 512, 8, 4]      float32 \nb8.conv1       2359808     16       [8, 512, 4, 2]      float32 \nb8             -           16       [8, 512, 4, 2]      float32 \nb4.mbstd       -           -        [8, 513, 4, 2]      float32 \nb4.conv        2364416     16       [8, 512, 4, 2]      float32 \nb4.fc          2097664     -        [8, 512]            float32 \nb4.out         513         -        [8, 1]              float32 \n---            ---         ---      ---                 ---     \nTotal          26885697    480      -                   -       \n\nSetting up augmentation...\nDistributing across 2 GPUs...\nSetting up training phases...\nExporting sample images...\nInitializing logs...\nTraining for 25000 kimg...\n\ntick 0     kimg 0.0      time 2m 00s       sec/tick 11.3    sec/kimg 705.15  maintenance 109.0  cpumem 5.10   gpumem 11.68  augment 0.000\ntick 1     kimg 4.0      time 8m 04s       sec/tick 337.1   sec/kimg 84.27   maintenance 26.3   cpumem 5.38   gpumem 4.90   augment 0.000\ntick 2     kimg 8.0      time 13m 41s      sec/tick 337.4   sec/kimg 84.36   maintenance 0.2    cpumem 5.38   gpumem 4.90   augment 0.000\ntick 3     kimg 12.0     time 19m 17s      sec/tick 335.9   sec/kimg 83.97   maintenance 0.2    cpumem 5.38   gpumem 4.90   augment 0.000\ntick 4     kimg 16.0     time 24m 54s      sec/tick 336.6   sec/kimg 84.16   maintenance 0.2    cpumem 5.38   gpumem 4.90   augment 0.000\ntick 5     kimg 20.0     time 30m 30s      sec/tick 336.1   sec/kimg 84.02   maintenance 0.2    cpumem 5.38   gpumem 4.90   augment 0.000\n","output_type":"stream"}]},{"cell_type":"code","source":"!curl -H \"Authorization: Bearer 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\" -X POST -F \"file=@/kaggle/working/stylegan2-ada-pytorch/training_results/sg2/00007-croppedhumans-rectangle-mirror-shhq2-noaug-resumecustom/network-snapshot-000080.pkl\" -F \"repo_name=nudezStyleGANhuman\" https://huggingface.co/api/models/fattyflakie/nudezStyleGANhuman/upload/main/currentModel.pkl","metadata":{"execution":{"iopub.status.busy":"2023-01-09T00:02:22.815137Z","iopub.execute_input":"2023-01-09T00:02:22.815449Z","iopub.status.idle":"2023-01-09T00:02:46.837244Z","shell.execute_reply.started":"2023-01-09T00:02:22.815417Z","shell.execute_reply":"2023-01-09T00:02:46.836377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls /kaggle/working/stylegan2-ada-pytorch/training_results/sg2/00007-croppedhumans-rectangle-mirror-shhq2-noaug-resumecustom","metadata":{"execution":{"iopub.status.busy":"2023-01-09T02:56:11.393671Z","iopub.execute_input":"2023-01-09T02:56:11.393941Z","iopub.status.idle":"2023-01-09T02:56:12.366671Z","shell.execute_reply.started":"2023-01-09T02:56:11.393910Z","shell.execute_reply":"2023-01-09T02:56:12.365780Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"events.out.tfevents.1673215173.b933b5de8001.552.0  network-snapshot-000000.pkl\nfakes000000.png\t\t\t\t\t   network-snapshot-000040.pkl\nfakes000040.png\t\t\t\t\t   network-snapshot-000080.pkl\nfakes000080.png\t\t\t\t\t   reals.png\nfakes_init.png\t\t\t\t\t   stats.jsonl\nlog.txt\t\t\t\t\t\t   training_options.json\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Preprocessing Dataset","metadata":{}},{"cell_type":"code","source":"!ls /kaggle/working/humanbgr/imgs/","metadata":{"execution":{"iopub.status.busy":"2023-01-09T03:08:17.806003Z","iopub.execute_input":"2023-01-09T03:08:17.806925Z","iopub.status.idle":"2023-01-09T03:08:18.790356Z","shell.execute_reply.started":"2023-01-09T03:08:17.806879Z","shell.execute_reply":"2023-01-09T03:08:18.789466Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"12037101_016_cdb6.jpg  42572064_015_23e6.jpg  67269698_016_bd27.jpg\n13457618_013_07a6.jpg  42572064_016_bb93.jpg  69609030_040_b0ec.jpg\n14196665_013_2663.jpg  42572064_017_2fa4.jpg  69946980_014_662f.jpg\n15339832_012_a23a.jpg  42774937_014_9e51.jpg  72760899_015_b093.jpg\n15339832_014_f4c9.jpg  42774937_015_9e51.jpg  79143841_007_7828.jpg\n15478173_013_ef69.jpg  42774937_016_c05f.jpg  79143841_008_6185.jpg\n15550288_013_7a6c.jpg  43248918_014_a182.jpg  79143841_009_a613.jpg\n15550288_016_826f.jpg  45100629_008_6712.jpg  79143841_013_e9a6.jpg\n16739062_015_1d98.jpg  45726284_014_83eb.jpg  79143841_014_ed64.jpg\n16739062_016_428f.jpg  45726284_016_ef1d.jpg  79143841_015_ed3d.jpg\n16884786_003_0917.jpg  45825033_014_996c.jpg  79143841_016_cf8b.jpg\n16884786_007_13c7.jpg  45825033_016_e8e5.jpg  81803053_040_442b.jpg\n18467611_007_ab44.jpg  46897363_012_6434.jpg  82133880_014_89fb.jpg\n18467611_011_2726.jpg  48480850_016_991b.jpg  82133880_015_5071.jpg\n19617467_014_9cd5.jpg  48480850_017_902c.jpg  82133880_016_9c87.jpg\n22689590_016_0113.jpg  48963953_014_5f86.jpg  84229342_010_f90c.jpg\n22689590_017_b917.jpg  48963953_015_4d38.jpg  84421036_014_2a3f.jpg\n23088616_017_c4e2.jpg  48963953_016_4432.jpg  84421036_016_1400.jpg\n23088616_031_3803.jpg  50595192_015_5664.jpg  84760830_015_0aab.jpg\n23096690_014_1fd2.jpg  50595192_016_ec21.jpg  85596126_013_4faa.jpg\n23096690_016_d26a.jpg  51698859_012_d8fc.jpg  85596126_014_53f3.jpg\n26386860_016_1f5d.jpg  51698859_017_8e9a.jpg  88665928_013_77d8.jpg\n28642319_014_af32.jpg  52296263_012_8b66.jpg  88665928_014_627e.jpg\n28642319_015_902e.jpg  53463828_013_6f49.jpg  88665928_016_c8d2.jpg\n28642319_016_5d73.jpg  53463828_015_3969.jpg  89280676_013_8dd5.jpg\n29664518_014_343a.jpg  54709282_006_d8ec.jpg  89280676_014_3e59.jpg\n29664518_016_9b6a.jpg  56705691_014_2fd9.jpg  89280676_016_f3f6.jpg\n32833442_013_13eb.jpg  57346984_013_802e.jpg  90320173_014_344a.jpg\n32833442_015_9531.jpg  61654743_014_f780.jpg  90320173_015_60d7.jpg\n34127347_014_99ef.jpg  61654743_015_529e.jpg  90320173_016_3921.jpg\n34127347_017_27e4.jpg  61654743_016_529e.jpg  92806689_014_e8be.jpg\n34620908_009_7a29.jpg  62006448_014_3ee1.jpg  92806689_015_eb49.jpg\n34620908_014_b0fc.jpg  62006448_015_62c6.jpg  92806689_016_aa31.jpg\n34620908_015_b0fc.jpg  62006448_016_62c6.jpg  93283547_013_4259.jpg\n34620908_016_e84d.jpg  62195856_015_3483.jpg  93283547_014_9a1c.jpg\n35778465_016_8b6d.jpg  62467072_015_9fe4.jpg  93283547_016_39ea.jpg\n39296507_013_927e.jpg  62467072_016_9fe4.jpg  94856153_012_7c47.jpg\n39296507_014_927e.jpg  62467072_017_8c4d.jpg  94856153_013_7fdb.jpg\n39296507_015_ceae.jpg  63952700_014_1220.jpg  94856153_015_294a.jpg\n39296507_016_a816.jpg  64854531_015_e291.jpg  95499651_014_890c.jpg\n39872672_014_01c4.jpg  64854531_017_3881.jpg  95499651_016_98c6.jpg\n39872672_015_7e4e.jpg  65774720_014_8e78.jpg  96548910_014_ff1c.jpg\n39872672_016_1e38.jpg  65774720_016_4178.jpg  96548910_016_321b.jpg\n39977319_016_d08a.jpg  66124643_013_f114.jpg  99408255_012_7eec.jpg\n41682693_014_af60.jpg  66124643_014_3fe8.jpg  girl2.jpg\n41771082_014_fd35.jpg  66124643_016_0ee8.jpg  girl3.jpg\n41771082_016_af45.jpg  66505490_014_ca6b.jpg\n","output_type":"stream"}]},{"cell_type":"code","source":"!python train.py --help","metadata":{"execution":{"iopub.status.busy":"2022-02-19T20:22:47.038824Z","iopub.execute_input":"2022-02-19T20:22:47.039105Z","iopub.status.idle":"2022-02-19T20:22:49.890973Z","shell.execute_reply.started":"2022-02-19T20:22:47.039075Z","shell.execute_reply":"2022-02-19T20:22:49.890042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training will take a long time, make sure you have enough GPU hours available (minimum 10 hours required for good results)\nDo !python train.py --help to see the different optional arguments available.\n\nResume from your previous checkpoint of notebook times out. Get output model from: ./stylegan2-ada-pytorch/results","metadata":{}},{"cell_type":"code","source":"!python train.py --outdir ./results --snap=4 --gpus=2 --cfg=auto --data=./datasets/girls2.zip --augpipe=\"bg\" --mirror=True --metrics=None --resume=/kaggle/working/stylegan2-ada-pytorch/training_results/sg2/00000-croppedhumans-rectangle-mirror-shhq2-noaug-resumecustom/network-snapshot-000048.pkl --augpipe=\"bg\" --kimg=145","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!cd /kaggle/working/stylegan2-ada-pytorch/results/00002-girls-mirror-paper512-kimg145-bg-resumecustom && zip -r /kaggle/working/output_images.zip . >/dev/null\n!curl --progress-bar --upload-file /kaggle/working/stylegan2-ada-pytorch/training_results/sg2/00007-croppedhumans-rectangle-mirror-shhq2-noaug-resumecustom/fakes000080.png https://transfer.sh/fakes000080.png -H \"Max-Days: 1\"","metadata":{"execution":{"iopub.status.busy":"2023-01-08T23:58:03.815845Z","iopub.execute_input":"2023-01-08T23:58:03.816447Z","iopub.status.idle":"2023-01-08T23:59:11.976261Z","shell.execute_reply.started":"2023-01-08T23:58:03.816407Z","shell.execute_reply":"2023-01-08T23:59:11.975303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!curl --progress-bar --upload-file /kaggle/working/stylegan2-ada-pytorch/training_results/sg2/00007-croppedhumans-rectangle-mirror-shhq2-noaug-resumecustom/network-snapshot-000080.pkl https://transfer.sh/network-snapshot-000080.pkl -H \"Max-Days: 5\"","metadata":{"execution":{"iopub.status.busy":"2023-01-08T10:23:49.871500Z","iopub.execute_input":"2023-01-08T10:23:49.871809Z","iopub.status.idle":"2023-01-08T10:53:01.220304Z","shell.execute_reply.started":"2023-01-08T10:23:49.871775Z","shell.execute_reply":"2023-01-08T10:53:01.219461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls /kaggle/working/stylegan2-ada-pytorch/training_results/sg2/00002-croppedhumans-rectangle-mirror-shhq2-noaug-resumecustom/","metadata":{"execution":{"iopub.status.busy":"2023-01-09T00:00:02.283283Z","iopub.execute_input":"2023-01-09T00:00:02.283595Z","iopub.status.idle":"2023-01-09T00:00:03.237622Z","shell.execute_reply.started":"2023-01-09T00:00:02.283554Z","shell.execute_reply":"2023-01-09T00:00:03.236730Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!rm /kaggle/working/stylegan2-ada-pytorch/training_results/sg2/00006-croppedhumans-rectangle-mirror-shhq2-noaug-resumecustom/*","metadata":{"execution":{"iopub.status.busy":"2023-01-09T00:00:53.673796Z","iopub.execute_input":"2023-01-09T00:00:53.674147Z","iopub.status.idle":"2023-01-09T00:00:54.630009Z","shell.execute_reply.started":"2023-01-09T00:00:53.674103Z","shell.execute_reply":"2023-01-09T00:00:54.628964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Stopping training as example notebook","metadata":{}}]}