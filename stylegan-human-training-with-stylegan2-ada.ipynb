{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Train your own GAN in a few lines of code!\nSee original StyleGAN2 ADA github repo here: https://github.com/NVlabs/stylegan2-ada-pytorch\n\nBy using a pre-trained model along with data heavy data augmentation, we can train our own GAN with a very limited dataset (<1000 images). \n\nMake sure you have a GPU runtime!","metadata":{}},{"cell_type":"code","source":"!pip install pyspng ninja imageio-ffmpeg==0.4.3\n#!git clone https://github.com/NVlabs/stylegan2-ada-pytorch\n!pip install gdown \n%cd /kaggle/working/stylegan2-ada-pytorch","metadata":{"execution":{"iopub.status.busy":"2023-01-09T23:29:38.002772Z","iopub.execute_input":"2023-01-09T23:29:38.003117Z","iopub.status.idle":"2023-01-09T23:29:53.210677Z","shell.execute_reply.started":"2023-01-09T23:29:38.003021Z","shell.execute_reply":"2023-01-09T23:29:53.209697Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: pyspng in /opt/conda/lib/python3.7/site-packages (0.1.1)\nRequirement already satisfied: ninja in /opt/conda/lib/python3.7/site-packages (1.11.1)\nRequirement already satisfied: imageio-ffmpeg==0.4.3 in /opt/conda/lib/python3.7/site-packages (0.4.3)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from pyspng) (1.20.3)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\nRequirement already satisfied: gdown in /opt/conda/lib/python3.7/site-packages (4.6.0)\nRequirement already satisfied: requests[socks] in /opt/conda/lib/python3.7/site-packages (from gdown) (2.26.0)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.7/site-packages (from gdown) (4.10.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from gdown) (3.4.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from gdown) (4.62.3)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from gdown) (1.16.0)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.7/site-packages (from beautifulsoup4->gdown) (2.3.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (3.1)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (1.26.7)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (2021.10.8)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (2.0.9)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (1.7.1)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n/kaggle/working/stylegan2-ada-pytorch\n","output_type":"stream"}]},{"cell_type":"code","source":"%cd /kaggle/working/\n!git clone https://github.com/stylegan-human/StyleGAN-Human\n#!wget 'https://cdn-lfs.huggingface.co/repos/1a/8c/1a8c83e1fa088d576d8201186ec45a38de5bcc3dfdbafcea86e5b3afa9362d1b/7af33f638e3b3aba7bf99456eec3e9d4a022d8a7dd67683a3605a7dd37665a3b?response-content-disposition=attachment%3B%20filename%3D%22stylegan_human_v2_512.pkl%22&Expires=1673415751&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzFhLzhjLzFhOGM4M2UxZmEwODhkNTc2ZDgyMDExODZlYzQ1YTM4ZGU1YmNjM2RmZGJhZmNlYTg2ZTViM2FmYTkzNjJkMWIvN2FmMzNmNjM4ZTNiM2FiYTdiZjk5NDU2ZWVjM2U5ZDRhMDIyZDhhN2RkNjc2ODNhMzYwNWE3ZGQzNzY2NWEzYj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPWF0dGFjaG1lbnQlM0IlMjBmaWxlbmFtZSUzRCUyMnN0eWxlZ2FuX2h1bWFuX3YyXzUxMi5wa2wlMjIiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2NzM0MTU3NTF9fX1dfQ__&Signature=dHvc7yaTAUB5NtA1vwnogSn4LXxufWERMQ4HJviNMs-VF-ZhzDTdGGIKC7m10yvRnL1QyIALkiIc~4JJMBiQS1Q0ghzM5ai3bNBnUOiiKkw~ZLNpoy3~4B3SXly5tgF8qZKDHkirna5IFyvLLd~mAbpubhGC1MSoHdv5spYQE4DSbyPFWRjz6LAz4Qp9TzGMN~s93gMi-eGA08DttzNXGT4n7G4RQl0YMJvcc-vVoBW7OjlS8yIAZpAlE~05bGQY9p8ECviHFz-FCQJ5TKn4NbCrtAKoXsRz9OjTtIXczD7Jg4oNIW9qXq3vZIAg~awElidutrX925BeP5zuTjIZRg__&Key-Pair-Id=KVTP0A1DKRTAX' -O /kaggle/working/styleganhuman.pkl","metadata":{"execution":{"iopub.status.busy":"2023-01-09T23:32:28.067771Z","iopub.execute_input":"2023-01-09T23:32:28.068478Z","iopub.status.idle":"2023-01-09T23:32:29.084510Z","shell.execute_reply.started":"2023-01-09T23:32:28.068431Z","shell.execute_reply":"2023-01-09T23:32:29.083549Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"/kaggle/working\nfatal: destination path 'StyleGAN-Human' already exists and is not an empty directory.\n","output_type":"stream"}]},{"cell_type":"code","source":"!cp /kaggle/working/StyleGAN-Human/training_scripts/sg2/training/networks.py /kaggle/working/stylegan2-ada-pytorch/training/networks.py","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd /kaggle/working/stylegan2-ada-pytorch","metadata":{"execution":{"iopub.status.busy":"2023-01-10T00:01:09.098137Z","iopub.execute_input":"2023-01-10T00:01:09.099815Z","iopub.status.idle":"2023-01-10T00:01:09.110618Z","shell.execute_reply.started":"2023-01-10T00:01:09.099753Z","shell.execute_reply":"2023-01-10T00:01:09.109659Z"},"trusted":true},"execution_count":59,"outputs":[{"name":"stdout","text":"/kaggle/working/stylegan2-ada-pytorch\n","output_type":"stream"}]},{"cell_type":"code","source":"!python alignment.py --image-folder /kaggle/input/1024humanset/ --output-folder /kaggle/working/aligned_image/","metadata":{"execution":{"iopub.status.busy":"2023-01-10T00:00:49.226836Z","iopub.execute_input":"2023-01-10T00:00:49.227461Z","iopub.status.idle":"2023-01-10T00:00:52.384591Z","shell.execute_reply.started":"2023-01-10T00:00:49.227403Z","shell.execute_reply":"2023-01-10T00:00:52.383572Z"},"trusted":true},"execution_count":57,"outputs":[{"name":"stdout","text":"Error: Can not import paddle core while this file exists: /opt/conda/lib/python3.7/site-packages/paddle/fluid/libpaddle.so\nTraceback (most recent call last):\n  File \"alignment.py\", line 22, in <module>\n    from PP_HumanSeg.deploy.infer import Predictor as PP_HumenSeg_Predictor\n  File \"/kaggle/working/StyleGAN-Human/PP_HumanSeg/deploy/infer.py\", line 25, in <module>\n    import paddle\n  File \"/opt/conda/lib/python3.7/site-packages/paddle/__init__.py\", line 25, in <module>\n    from .framework import monkey_patch_variable\n  File \"/opt/conda/lib/python3.7/site-packages/paddle/framework/__init__.py\", line 17, in <module>\n    from . import random  # noqa: F401\n  File \"/opt/conda/lib/python3.7/site-packages/paddle/framework/random.py\", line 16, in <module>\n    import paddle.fluid as fluid\n  File \"/opt/conda/lib/python3.7/site-packages/paddle/fluid/__init__.py\", line 36, in <module>\n    from . import framework\n  File \"/opt/conda/lib/python3.7/site-packages/paddle/fluid/framework.py\", line 37, in <module>\n    from . import core\n  File \"/opt/conda/lib/python3.7/site-packages/paddle/fluid/core.py\", line 304, in <module>\n    raise e\n  File \"/opt/conda/lib/python3.7/site-packages/paddle/fluid/core.py\", line 249, in <module>\n    from . import libpaddle\nImportError: libcudart.so.10.2: cannot open shared object file: No such file or directory\n","output_type":"stream"}]},{"cell_type":"code","source":"!python -m pip download paddlepaddle-gpu==2.3.1","metadata":{"execution":{"iopub.status.busy":"2023-01-09T23:58:16.872007Z","iopub.execute_input":"2023-01-09T23:58:16.872811Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\nCollecting paddlepaddle-gpu==2.3.1\n  Downloading paddlepaddle_gpu-2.3.1-cp37-cp37m-manylinux1_x86_64.whl (393.8 MB)\n     |██████████████████████▏         | 272.5 MB 5.4 MB/s eta 0:00:23 ","output_type":"stream"}]},{"cell_type":"code","source":"!gdown '1EULkcH_hhSU28qVc1jSJpCh2hGOrzpjK'","metadata":{"execution":{"iopub.status.busy":"2023-01-09T23:40:34.378439Z","iopub.execute_input":"2023-01-09T23:40:34.379318Z","iopub.status.idle":"2023-01-09T23:40:41.972510Z","shell.execute_reply.started":"2023-01-09T23:40:34.379270Z","shell.execute_reply":"2023-01-09T23:40:41.971617Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"Downloading...\nFrom: https://drive.google.com/uc?id=1EULkcH_hhSU28qVc1jSJpCh2hGOrzpjK\nTo: /kaggle/working/StyleGAN-Human/body_pose_model.pth\n100%|████████████████████████████████████████| 209M/209M [00:05<00:00, 36.7MB/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-01-09T23:41:28.073799Z","iopub.execute_input":"2023-01-09T23:41:28.074649Z","iopub.status.idle":"2023-01-09T23:41:29.054890Z","shell.execute_reply.started":"2023-01-09T23:41:28.074596Z","shell.execute_reply":"2023-01-09T23:41:29.053858Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"!python trainh.py --outdir=training_results/sg2/ --metrics=None --data=/kaggle/input/1024humanset/ --gpus=2 --aug=noaug --mirror=1 --snap=2 --cfg=shhq --square=False --resume=/kaggle/working/stylegan2-ada-pytorch/training_results/sg2/00021--rectangle-mirror-shhq2-noaug-resumecustom/network-snapshot-000012.pkl","metadata":{"execution":{"iopub.status.busy":"2023-01-10T00:01:12.215986Z","iopub.execute_input":"2023-01-10T00:01:12.216740Z","iopub.status.idle":"2023-01-10T00:02:47.942949Z","shell.execute_reply.started":"2023-01-10T00:01:12.216699Z","shell.execute_reply":"2023-01-10T00:02:47.941922Z"},"trusted":true},"execution_count":60,"outputs":[{"name":"stdout","text":"square :  False\ndesc:  \n\nTraining options:\n{\n  \"num_gpus\": 2,\n  \"image_snapshot_ticks\": 2,\n  \"network_snapshot_ticks\": 2,\n  \"metrics\": [],\n  \"random_seed\": 0,\n  \"training_set_kwargs\": {\n    \"class_name\": \"training.dataset.ImageFolderDataset\",\n    \"path\": \"/kaggle/input/1024humanset/\",\n    \"use_labels\": false,\n    \"max_size\": 197,\n    \"xflip\": true,\n    \"square\": false,\n    \"resolution\": 1024\n  },\n  \"data_loader_kwargs\": {\n    \"pin_memory\": true,\n    \"num_workers\": 3,\n    \"prefetch_factor\": 2\n  },\n  \"G_kwargs\": {\n    \"class_name\": \"training.networks.Generator\",\n    \"z_dim\": 512,\n    \"w_dim\": 512,\n    \"mapping_kwargs\": {\n      \"num_layers\": 8\n    },\n    \"synthesis_kwargs\": {\n      \"channel_base\": 32768,\n      \"channel_max\": 512,\n      \"num_fp16_res\": 4,\n      \"conv_clamp\": 256\n    },\n    \"square\": false\n  },\n  \"D_kwargs\": {\n    \"class_name\": \"training.networks.Discriminator\",\n    \"block_kwargs\": {},\n    \"mapping_kwargs\": {},\n    \"epilogue_kwargs\": {\n      \"mbstd_group_size\": 4\n    },\n    \"square\": false,\n    \"channel_base\": 32768,\n    \"channel_max\": 512,\n    \"num_fp16_res\": 4,\n    \"conv_clamp\": 256\n  },\n  \"G_opt_kwargs\": {\n    \"class_name\": \"torch.optim.Adam\",\n    \"lr\": 0.002,\n    \"betas\": [\n      0,\n      0.99\n    ],\n    \"eps\": 1e-08\n  },\n  \"D_opt_kwargs\": {\n    \"class_name\": \"torch.optim.Adam\",\n    \"lr\": 0.002,\n    \"betas\": [\n      0,\n      0.99\n    ],\n    \"eps\": 1e-08\n  },\n  \"loss_kwargs\": {\n    \"class_name\": \"training.loss.StyleGAN2Loss\",\n    \"r1_gamma\": 26.2144\n  },\n  \"total_kimg\": 25000,\n  \"batch_size\": 8,\n  \"batch_gpu\": 4,\n  \"ema_kimg\": 2.5,\n  \"ema_rampup\": null,\n  \"resume_pkl\": \"/kaggle/working/stylegan2-ada-pytorch/training_results/sg2/00021--rectangle-mirror-shhq2-noaug-resumecustom/network-snapshot-000012.pkl\",\n  \"ada_kimg\": 100,\n  \"run_dir\": \"training_results/sg2/00023--rectangle-mirror-shhq2-noaug-resumecustom\"\n}\n\nOutput directory:   training_results/sg2/00023--rectangle-mirror-shhq2-noaug-resumecustom\nTraining data:      /kaggle/input/1024humanset/\nTraining duration:  25000 kimg\nNumber of GPUs:     2\nNumber of images:   197\nImage resolution:   1024\nConditional model:  False\nDataset x-flips:    True\n\nCreating output directory...\nLaunching processes...\nLoading training set...\n/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  cpuset_checked))\n/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  cpuset_checked))\n\nNum images:  394\nImage shape: [3, 1024, 512]\nLabel shape: [0]\n\nConstructing networks...\nResuming from \"/kaggle/working/stylegan2-ada-pytorch/training_results/sg2/00021--rectangle-mirror-shhq2-noaug-resumecustom/network-snapshot-000012.pkl\"\nSetting up PyTorch plugin \"bias_act_plugin\"... Done.\nSetting up PyTorch plugin \"upfirdn2d_plugin\"... Done.\n\nGenerator              Parameters  Buffers  Output shape        Datatype\n---                    ---         ---      ---                 ---     \nmapping.fc0            262656      -        [4, 512]            float32 \nmapping.fc1            262656      -        [4, 512]            float32 \nmapping.fc2            262656      -        [4, 512]            float32 \nmapping.fc3            262656      -        [4, 512]            float32 \nmapping.fc4            262656      -        [4, 512]            float32 \nmapping.fc5            262656      -        [4, 512]            float32 \nmapping.fc6            262656      -        [4, 512]            float32 \nmapping.fc7            262656      -        [4, 512]            float32 \nmapping                -           512      [4, 18, 512]        float32 \nsynthesis.b4.conv1     2622465     24       [4, 512, 4, 2]      float32 \nsynthesis.b4.torgb     264195      -        [4, 3, 4, 2]        float32 \nsynthesis.b4:0         4096        16       [4, 512, 4, 2]      float32 \nsynthesis.b4:1         -           -        [4, 512, 4, 2]      float32 \nsynthesis.b8.conv0     2622465     48       [4, 512, 8, 4]      float32 \nsynthesis.b8.conv1     2622465     48       [4, 512, 8, 4]      float32 \nsynthesis.b8.torgb     264195      -        [4, 3, 8, 4]        float32 \nsynthesis.b8:0         -           16       [4, 512, 8, 4]      float32 \nsynthesis.b8:1         -           -        [4, 512, 8, 4]      float32 \nsynthesis.b16.conv0    2622465     144      [4, 512, 16, 8]     float32 \nsynthesis.b16.conv1    2622465     144      [4, 512, 16, 8]     float32 \nsynthesis.b16.torgb    264195      -        [4, 3, 16, 8]       float32 \nsynthesis.b16:0        -           16       [4, 512, 16, 8]     float32 \nsynthesis.b16:1        -           -        [4, 512, 16, 8]     float32 \nsynthesis.b32.conv0    2622465     528      [4, 512, 32, 16]    float32 \nsynthesis.b32.conv1    2622465     528      [4, 512, 32, 16]    float32 \nsynthesis.b32.torgb    264195      -        [4, 3, 32, 16]      float32 \nsynthesis.b32:0        -           16       [4, 512, 32, 16]    float32 \nsynthesis.b32:1        -           -        [4, 512, 32, 16]    float32 \nsynthesis.b64.conv0    2622465     2064     [4, 512, 64, 32]    float32 \nsynthesis.b64.conv1    2622465     2064     [4, 512, 64, 32]    float32 \nsynthesis.b64.torgb    264195      -        [4, 3, 64, 32]      float32 \nsynthesis.b64:0        -           16       [4, 512, 64, 32]    float32 \nsynthesis.b64:1        -           -        [4, 512, 64, 32]    float32 \nsynthesis.b128.conv0   1442561     8208     [4, 256, 128, 64]   float16 \nsynthesis.b128.conv1   721409      8208     [4, 256, 128, 64]   float16 \nsynthesis.b128.torgb   132099      -        [4, 3, 128, 64]     float16 \nsynthesis.b128:0       -           16       [4, 256, 128, 64]   float16 \nsynthesis.b128:1       -           -        [4, 256, 128, 64]   float32 \nsynthesis.b256.conv0   426369      32784    [4, 128, 256, 128]  float16 \nsynthesis.b256.conv1   213249      32784    [4, 128, 256, 128]  float16 \nsynthesis.b256.torgb   66051       -        [4, 3, 256, 128]    float16 \nsynthesis.b256:0       -           16       [4, 128, 256, 128]  float16 \nsynthesis.b256:1       -           -        [4, 128, 256, 128]  float32 \nsynthesis.b512.conv0   139457      131088   [4, 64, 512, 256]   float16 \nsynthesis.b512.conv1   69761       131088   [4, 64, 512, 256]   float16 \nsynthesis.b512.torgb   33027       -        [4, 3, 512, 256]    float16 \nsynthesis.b512:0       -           16       [4, 64, 512, 256]   float16 \nsynthesis.b512:1       -           -        [4, 64, 512, 256]   float32 \nsynthesis.b1024.conv0  51297       524304   [4, 32, 1024, 512]  float16 \nsynthesis.b1024.conv1  25665       524304   [4, 32, 1024, 512]  float16 \nsynthesis.b1024.torgb  16515       -        [4, 3, 1024, 512]   float16 \nsynthesis.b1024:0      -           16       [4, 32, 1024, 512]  float16 \nsynthesis.b1024:1      -           -        [4, 32, 1024, 512]  float32 \n---                    ---         ---      ---                 ---     \nTotal                  30365964    1399016  -                   -       \n\n\nDiscriminator  Parameters  Buffers  Output shape        Datatype\n---            ---         ---      ---                 ---     \nb1024.fromrgb  128         16       [4, 32, 1024, 512]  float16 \nb1024.skip     2048        16       [4, 64, 512, 256]   float16 \nb1024.conv0    9248        16       [4, 32, 1024, 512]  float16 \nb1024.conv1    18496       16       [4, 64, 512, 256]   float16 \nb1024          -           16       [4, 64, 512, 256]   float16 \nb512.skip      8192        16       [4, 128, 256, 128]  float16 \nb512.conv0     36928       16       [4, 64, 512, 256]   float16 \nb512.conv1     73856       16       [4, 128, 256, 128]  float16 \nb512           -           16       [4, 128, 256, 128]  float16 \nb256.skip      32768       16       [4, 256, 128, 64]   float16 \nb256.conv0     147584      16       [4, 128, 256, 128]  float16 \nb256.conv1     295168      16       [4, 256, 128, 64]   float16 \nb256           -           16       [4, 256, 128, 64]   float16 \nb128.skip      131072      16       [4, 512, 64, 32]    float16 \nb128.conv0     590080      16       [4, 256, 128, 64]   float16 \nb128.conv1     1180160     16       [4, 512, 64, 32]    float16 \nb128           -           16       [4, 512, 64, 32]    float16 \nb64.skip       262144      16       [4, 512, 32, 16]    float32 \nb64.conv0      2359808     16       [4, 512, 64, 32]    float32 \nb64.conv1      2359808     16       [4, 512, 32, 16]    float32 \nb64            -           16       [4, 512, 32, 16]    float32 \nb32.skip       262144      16       [4, 512, 16, 8]     float32 \nb32.conv0      2359808     16       [4, 512, 32, 16]    float32 \nb32.conv1      2359808     16       [4, 512, 16, 8]     float32 \nb32            -           16       [4, 512, 16, 8]     float32 \nb16.skip       262144      16       [4, 512, 8, 4]      float32 \nb16.conv0      2359808     16       [4, 512, 16, 8]     float32 \nb16.conv1      2359808     16       [4, 512, 8, 4]      float32 \nb16            -           16       [4, 512, 8, 4]      float32 \nb8.skip        262144      16       [4, 512, 4, 2]      float32 \nb8.conv0       2359808     16       [4, 512, 8, 4]      float32 \nb8.conv1       2359808     16       [4, 512, 4, 2]      float32 \nb8             -           16       [4, 512, 4, 2]      float32 \nb4.mbstd       -           -        [4, 513, 4, 2]      float32 \nb4.conv        2364416     16       [4, 512, 4, 2]      float32 \nb4.fc          2097664     -        [4, 512]            float32 \nb4.out         513         -        [4, 1]              float32 \n---            ---         ---      ---                 ---     \nTotal          26915361    544      -                   -       \n\nSetting up augmentation...\nDistributing across 2 GPUs...\nSetting up training phases...\nExporting sample images...\nInitializing logs...\nTraining for 25000 kimg...\n\ntick 0     kimg 0.0      time 1m 20s       sec/tick 9.3     sec/kimg 1157.06 maintenance 70.6   cpumem 5.14   gpumem 9.28   augment 0.000\n^C\n\nAborted!\n","output_type":"stream"}]},{"cell_type":"code","source":"!curl -H \"Authorization: Bearer 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\" -X POST -F \"file=@/kaggle/working/stylegan2-ada-pytorch/training_results/sg2/00017--rectangle-mirror-shhq2-noaug-resumecustom/network-snapshot-000400.pkl\" -F \"repo_name=nudezStyleGANhuman\" https://huggingface.co/api/models/fattyflakie/nudezStyleGANhuman/upload/main/currentModel.pkl","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls /kaggle/working/stylegan2-ada-pytorch/training_results/sg2/00021--rectangle-mirror-shhq2-noaug-resumecustom/network-snapshot-000012.pkl","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Preprocessing Dataset","metadata":{}},{"cell_type":"code","source":"!ls /kaggle/working/stylegan2-ada-pytorch/training_results/sg2/","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python train.py --help","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training will take a long time, make sure you have enough GPU hours available (minimum 10 hours required for good results)\nDo !python train.py --help to see the different optional arguments available.\n\nResume from your previous checkpoint of notebook times out. Get output model from: ./stylegan2-ada-pytorch/results","metadata":{}},{"cell_type":"code","source":"!rm -r /kaggle/working/stylegan2-ada-pytorch/training_results/sg2/00017--rectangle-mirror-shhq2-noaug-resumecustom/","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!cd /kaggle/working/stylegan2-ada-pytorch/results/00002-girls-mirror-paper512-kimg145-bg-resumecustom && zip -r /kaggle/working/output_images.zip . >/dev/null\n!curl --progress-bar --upload-file /kaggle/working/stylegan2-ada-pytorch/training_results/sg2/00014--rectangle-mirror-shhq2-noaug-resumecustom/fakes000080.png https://transfer.sh/fakes000080.png -H \"Max-Days: 1\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!curl --progress-bar --upload-file /kaggle/working/stylegan2-ada-pytorch/training_results/sg2/00021--rectangle-mirror-shhq2-noaug-resumecustom/fakes000012.png https://transfer.sh/seddd1d.png -H \"Max-Days: 5\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!gdown '1FlAb1rYa0r_--Zj_ML8e6shmaF28hQb5'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!wget 'https://cdn-lfs.huggingface.co/repos/47/1e/471ef24d71ed5adcaf1c4e0bc8c2ffe289f037a51d9c5687bc571ae3c164212f/4469f35200a6a52a749a3ab5e8a46d38f9d35d53bc7368a6b62201b6979c7301?response-content-disposition=attachment%3B%20filename%3D%221024Model.pkl%22&Expires=1673561198&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzQ3LzFlLzQ3MWVmMjRkNzFlZDVhZGNhZjFjNGUwYmM4YzJmZmUyODlmMDM3YTUxZDljNTY4N2JjNTcxYWUzYzE2NDIxMmYvNDQ2OWYzNTIwMGE2YTUyYTc0OWEzYWI1ZThhNDZkMzhmOWQzNWQ1M2JjNzM2OGE2YjYyMjAxYjY5NzljNzMwMT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPWF0dGFjaG1lbnQlM0IlMjBmaWxlbmFtZSUzRCUyMjEwMjRNb2RlbC5wa2wlMjIiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2NzM1NjExOTh9fX1dfQ__&Signature=Tk9k2jEpyV~GvYcZnyEGd~i-mYZ9k57q22hK3l19gGMzk110BS4uz79jvZjXKL4vh3xS5l~Bn2y~qxLARlWXcYqh4jhLBeLoo0eWSKMLPNtAilndEMXB3g1nGqFuUCE0SD4VDTKCr04ZCsEVxpVzf44grP9W8z~vv8a0hfJivaaMJEtj~tT7MNUQyRDXfXuKuoTriAKXCSYsFzrCHjf5YSDWkK1bnWhvpoaeKRXmcQN8dwGGZACy7aL2QfDYXzopp3NXlvJUPV2~K6lQKYRPWuTWDD9j1kLSiKJfEbr7J1DrvZDlRqrYJ5ULcAz6JFY3snY7cfxGyTi5gFVvJk~Y1g__&Key-Pair-Id=KVTP0A1DKRTAX' -O /kaggle/working/starter102f.pkl","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Stopping training as example notebook","metadata":{}}]}